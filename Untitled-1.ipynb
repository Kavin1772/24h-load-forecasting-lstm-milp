{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5dccc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 50000, Test size: 10000\n",
      "Classes: ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ENGR 413 — HW 3 Template (PyTorch + CIFAR-10)\n",
    "Fall 2025 • Starter code covering Q1–Q13\n",
    "Run cell-by-cell in a Jupyter Notebook or as a .py script.\n",
    "You may trim epochs to fit your hardware. Plots and logs are saved to ./outputs.\n",
    "\"\"\"\n",
    "\n",
    "# ===============\n",
    "# 0) Imports & Setup\n",
    "# ===============\n",
    "import os, math, time, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# =====================\n",
    "# 1) Data Preparation (Q1, Q2)\n",
    "# =====================\n",
    "CIFAR10_CLASSES = ('airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck')\n",
    "\n",
    "@dataclass\n",
    "class DataCfg:\n",
    "    batch_size: int = 128  # (Q8) you may change in experiments\n",
    "    num_workers: int = 2\n",
    "    data_root: str = \"./data\"\n",
    "\n",
    "CFG = DataCfg()\n",
    "\n",
    "# Q1.1: Load CIFAR10, compute global mean/std, and normalize\n",
    "\n",
    "def compute_cifar10_mean_std(root: str = CFG.data_root) -> Tuple[Tuple[float,...], Tuple[float,...]]:\n",
    "    tmp = torchvision.datasets.CIFAR10(root, train=True, download=True, transform=T.ToTensor())\n",
    "    loader = DataLoader(tmp, batch_size=512, shuffle=False, num_workers=CFG.num_workers)\n",
    "    n = 0\n",
    "    mean = torch.zeros(3)\n",
    "    M2 = torch.zeros(3)  # for online variance (Welford)\n",
    "    for x, _ in loader:\n",
    "        b = x.shape[0]\n",
    "        x = x.view(b, 3, -1)\n",
    "        batch_mean = x.mean(dim=(0, 2))\n",
    "        batch_var = x.var(dim=(0, 2), unbiased=False)\n",
    "        if n == 0:\n",
    "            mean = batch_mean\n",
    "            M2 = batch_var\n",
    "            n = b\n",
    "        else:\n",
    "            new_n = n + b\n",
    "            delta = batch_mean - mean\n",
    "            mean = mean + delta * (b / new_n)\n",
    "            M2 = (n * (M2 + (delta**2) * (b / new_n))) / new_n + (b * batch_var) / new_n\n",
    "            n = new_n\n",
    "    std = torch.sqrt(M2)\n",
    "    return tuple(mean.tolist()), tuple(std.tolist())\n",
    "\n",
    "\n",
    "# Build datasets and dataloaders\n",
    "\n",
    "def make_dataloaders(batch_size: int = CFG.batch_size) -> Tuple[DataLoader, DataLoader, Tuple[float,...], Tuple[float,...]]:\n",
    "    mean, std = compute_cifar10_mean_std()\n",
    "    # Q1.1: use transforms.Normalize with computed mean/std\n",
    "    train_tf = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    train_set = torchvision.datasets.CIFAR10(CFG.data_root, train=True, download=False, transform=train_tf)\n",
    "    test_set  = torchvision.datasets.CIFAR10(CFG.data_root, train=False, download=True, transform=test_tf)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=256, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader, mean, std\n",
    "\n",
    "# Q1.2 + Q1.3 (basic inspection helper)\n",
    "\n",
    "def describe_dataset(train_loader: DataLoader, test_loader: DataLoader):\n",
    "    n_train = len(train_loader.dataset)\n",
    "    n_test = len(test_loader.dataset)\n",
    "    print(f\"Train size: {n_train}, Test size: {n_test}\")\n",
    "    print(\"Classes:\", CIFAR10_CLASSES)\n",
    "\n",
    "# =====================\n",
    "# 2) Visualization (Q3)\n",
    "# =====================\n",
    "\n",
    "def show_grid(loader: DataLoader, nrow: int = 8, ncol: int = 8, unnormalize: Optional[Tuple[Tuple[float,...], Tuple[float,...]]] = None, savepath: str = \"outputs/visual_grid.png\"):\n",
    "    x, y = next(iter(loader))\n",
    "    x = x[: nrow*ncol]\n",
    "    y = y[: nrow*ncol]\n",
    "    if unnormalize is not None:\n",
    "        mean, std = [torch.tensor(v).view(1, -1, 1, 1) for v in unnormalize]\n",
    "        x = x * std + mean\n",
    "    grid = torchvision.utils.make_grid(x, nrow=ncol)\n",
    "    npimg = grid.permute(1, 2, 0).detach().cpu().numpy()\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.imshow(npimg)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"CIFAR-10 sample grid\")\n",
    "    plt.savefig(savepath, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved visualization to {savepath}\")\n",
    "\n",
    "# ======================================\n",
    "# 3) Models (Q4–Q7): CNN (+BN toggle) and MLP\n",
    "# ======================================\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    # For Q7 (initialize and move to device)\n",
    "    def __init__(self, hidden: int = 512, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*32*32, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, use_bn: bool = True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True)]\n",
    "        if use_bn:\n",
    "            layers.insert(1, nn.BatchNorm2d(out_ch))\n",
    "        layers += [nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True)]\n",
    "        if use_bn:\n",
    "            layers.insert(-1, nn.BatchNorm2d(out_ch))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Q4/Q5/Q6: customizable CNN with optional BatchNorm.\n",
    "    Args:\n",
    "      channels: list like [64, 128, 256]\n",
    "      use_bn: add BatchNorm after conv layers (Q5)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: List[int] = [64,128,256], use_bn: bool = True, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        c = [3] + channels\n",
    "        blocks = []\n",
    "        for i in range(len(channels)):\n",
    "            blocks += [ConvBlock(c[i], c[i+1], use_bn), nn.MaxPool2d(2)]\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(channels[-1], num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Hardware info (Q3.4) + Q7: initialize and move models to device\n",
    "\n",
    "def print_hardware_info():\n",
    "    print(\"Device:\", DEVICE)\n",
    "    if DEVICE.type == 'cuda':\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    elif DEVICE.type == 'mps':\n",
    "        print(\"Apple Silicon GPU via MPS backend\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "# ================================\n",
    "# 4) Training Utilities (Q8–Q9)\n",
    "# ================================\n",
    "\n",
    "def make_optimizer(params, name: str = \"sgd\", lr: float = 0.1, momentum: float = 0.9, weight_decay: float = 5e-4):\n",
    "    name = name.lower()\n",
    "    if name == \"sgd\":\n",
    "        return SGD(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif name in (\"sgdm\", \"sgd_momentum\", \"sgd+momentum\"):\n",
    "        return SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {name}\")\n",
    "\n",
    "\n",
    "def make_scheduler(optimizer, name: Optional[str] = None, epochs: int = 30):\n",
    "    if name is None:\n",
    "        return None\n",
    "    name = name.lower()\n",
    "    if name in (\"cosine\", \"cosineannealinglr\"):\n",
    "        return CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    elif name in (\"step\", \"steplr\"):\n",
    "        return StepLR(optimizer, step_size=max(1, epochs//3), gamma=0.1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler: {name}\")\n",
    "\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module):\n",
    "    model.eval()\n",
    "    tot_loss, tot_acc, n = 0.0, 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        b = y.size(0)\n",
    "        tot_loss += loss.item() * b\n",
    "        tot_acc  += (logits.argmax(1) == y).float().sum().item()\n",
    "        n += b\n",
    "    return tot_loss/n, tot_acc/n\n",
    "\n",
    "\n",
    "def train(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader,\n",
    "          epochs: int = 10, lr: float = 0.1, optimizer_name: str = \"sgdm\",\n",
    "          scheduler_name: Optional[str] = \"cosine\"):\n",
    "    \"\"\"Q4.3/4.4 + logging for Q5.1. Returns history dict.\"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = make_optimizer(model.parameters(), optimizer_name, lr=lr)\n",
    "    sched = make_scheduler(optim, scheduler_name, epochs)\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        ep_loss, ep_acc, n = 0.0, 0.0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            with torch.no_grad():\n",
    "                b = y.size(0)\n",
    "                ep_loss += loss.item() * b\n",
    "                ep_acc  += (logits.argmax(1) == y).float().sum().item()\n",
    "                n += b\n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "        tr_loss, tr_acc = ep_loss/n, ep_acc/n\n",
    "        te_loss, te_acc = evaluate(model, test_loader, criterion)\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"test_loss\"].append(te_loss)\n",
    "        history[\"test_acc\"].append(te_acc)\n",
    "        print(f\"Epoch {ep:02d}/{epochs} | train loss {tr_loss:.3f} acc {tr_acc*100:.1f}% | test loss {te_loss:.3f} acc {te_acc*100:.1f}%\")\n",
    "    return history\n",
    "\n",
    "# ==========================\n",
    "# 5) Plotting & Comparison\n",
    "# ==========================\n",
    "\n",
    "def plot_curves(histories: Dict[str, Dict[str, List[float]]], title: str, save_prefix: str):\n",
    "    # Q5.1: plot loss and accuracy per epoch\n",
    "    # One plot per metric to keep them readable\n",
    "    e = len(next(iter(histories.values()))[\"train_loss\"]) if histories else 0\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    for name, h in histories.items():\n",
    "        plt.plot(range(1, e+1), h[\"train_loss\"], label=f\"{name} - train\")\n",
    "        plt.plot(range(1, e+1), h[\"test_loss\"], label=f\"{name} - test\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(title + \" — Loss\"); plt.legend();\n",
    "    lp = f\"outputs/{save_prefix}_loss.png\"; plt.savefig(lp, bbox_inches='tight'); plt.close(); print(\"Saved:\", lp)\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    for name, h in histories.items():\n",
    "        plt.plot(range(1, e+1), [a*100 for a in h[\"train_acc\"]], label=f\"{name} - train\")\n",
    "        plt.plot(range(1, e+1), [a*100 for a in h[\"test_acc\"]], label=f\"{name} - test\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(title + \" — Accuracy\"); plt.legend();\n",
    "    ap = f\"outputs/{save_prefix}_acc.png\"; plt.savefig(ap, bbox_inches='tight'); plt.close(); print(\"Saved:\", ap)\n",
    "\n",
    "# =====================================\n",
    "# 6) Experiments for Questions 10–12\n",
    "# =====================================\n",
    "\n",
    "def run_all(epochs: int = 10, batch_size: int = 128, lr: float = 0.1):\n",
    "    # Make data\n",
    "    train_loader, test_loader, mean, std = make_dataloaders(batch_size)\n",
    "    describe_dataset(train_loader, test_loader)  # (Q2)\n",
    "    show_grid(train_loader, unnormalize=(mean, std))  # (Q3)\n",
    "    print_hardware_info()  # (Q3.4)\n",
    "\n",
    "    # Q7: Initialize MLP and move to device\n",
    "    mlp = SimpleMLP().to(DEVICE)\n",
    "\n",
    "    # Q4/5/6: CNNs with and without BatchNorm (and customizable channels)\n",
    "    cnn_bn    = SimpleCNN(channels=[64,128,256], use_bn=True).to(DEVICE)\n",
    "    cnn_nobn  = SimpleCNN(channels=[64,128,256], use_bn=False).to(DEVICE)\n",
    "\n",
    "    # ---- Train a lightweight set of epochs so the assignment can run quickly ----\n",
    "    # You can increase epochs for better accuracy if you have GPU time.\n",
    "    histories = {}\n",
    "\n",
    "    # (Q10) MLP vs CNN\n",
    "    print(\"\\n[Q10] Training MLP vs CNN (BN)\")\n",
    "    histories['MLP']    = train(mlp,    train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    histories['CNN+BN'] = train(cnn_bn, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    plot_curves({k: histories[k] for k in ['MLP','CNN+BN']}, title='MLP vs CNN', save_prefix='q10_mlp_vs_cnn')\n",
    "\n",
    "    # (Q11) CNN with vs without BatchNorm\n",
    "    print(\"\\n[Q11] Training CNN with vs without BatchNorm\")\n",
    "    # re-init models so they start fresh\n",
    "    cnn_bn    = SimpleCNN(channels=[64,128,256], use_bn=True).to(DEVICE)\n",
    "    cnn_nobn  = SimpleCNN(channels=[64,128,256], use_bn=False).to(DEVICE)\n",
    "    histories['CNN+BN_2']   = train(cnn_bn,   train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    histories['CNN_noBN_2'] = train(cnn_nobn, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    plot_curves({'CNN+BN': histories['CNN+BN_2'], 'CNN no BN': histories['CNN_noBN_2']}, title='CNN: BN vs no BN', save_prefix='q11_bn_vs_nobn')\n",
    "\n",
    "    # (Q12) Optimizers comparison on the same CNN architecture\n",
    "    print(\"\\n[Q12] Optimizer comparison (SGD vs SGD+Momentum)\")\n",
    "    cnn_opt_a = SimpleCNN(channels=[64,128,256], use_bn=True).to(DEVICE)\n",
    "    cnn_opt_b = SimpleCNN(channels=[64,128,256], use_bn=True).to(DEVICE)\n",
    "    histories['CNN_SGD']  = train(cnn_opt_a, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgd',  scheduler_name='cosine')\n",
    "    histories['CNN_SGDM'] = train(cnn_opt_b, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    plot_curves({'CNN SGD': histories['CNN_SGD'], 'CNN SGD+Momentum': histories['CNN_SGDM']}, title='Optimizers on CNN', save_prefix='q12_optims')\n",
    "\n",
    "    print(\"\\nDone. Figures saved under ./outputs. Use these when answering Q13 (overfitting signs, factors, etc.).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example fast run for CPU users; raise epochs to 20–30 on GPU for nicer curves\n",
    "    run_all(epochs=5, batch_size=CFG.batch_size, lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f0e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 50000, Test size: 10000\n",
      "Classes: ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ENGR 413 — HW 3 Template (PyTorch + CIFAR-10)\n",
    "Fall 2025 • Starter code covering Q1–Q13\n",
    "Run cell-by-cell in a Jupyter Notebook or as a .py script.\n",
    "You may trim epochs to fit your hardware. Plots and logs are saved to ./outputs.\n",
    "\"\"\"\n",
    "\n",
    "# ===============\n",
    "# 0) Imports & Setup\n",
    "# ===============\n",
    "import os, math, time, random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # safer in some notebook/Headless envs to prevent crashes\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "# Stability tweaks to avoid kernel crashes on some setups (e.g., macOS/MPS, Windows)\n",
    "os.environ.setdefault(\"PYTORCH_ENABLE_MPS_FALLBACK\", \"1\")  # lets ops fall back to CPU if unsupported\n",
    "torch.set_num_threads(max(1, (os.cpu_count() or 2)//2))  # keep CPU threads modest to reduce contention\n",
    "IS_MAC = platform.system() == \"Darwin\"\n",
    "\n",
    "# =====================\n",
    "# 1) Data Preparation (Q1, Q2)\n",
    "# =====================\n",
    "CIFAR10_CLASSES = ('airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck')\n",
    "\n",
    "@dataclass\n",
    "class DataCfg:\n",
    "    batch_size: int = 128  # (Q8) you may change in experiments\n",
    "    num_workers: int = 0   # SAFER default; we'll raise if CUDA is available\n",
    "    pin_memory: bool = False\n",
    "    data_root: str = \"./data\"\n",
    "\n",
    "CFG = DataCfg()\n",
    "# Bump workers/pin_memory only when CUDA is available (avoids macOS/Windows crashes)\n",
    "if DEVICE.type == \"cuda\":\n",
    "    CFG.num_workers = min(4, (os.cpu_count() or 2))\n",
    "    CFG.pin_memory = True\n",
    "else:\n",
    "    CFG.num_workers = 0\n",
    "    CFG.pin_memory = False\n",
    "\n",
    "# Q1.1: Load CIFAR10, compute global mean/std, and normalize\n",
    "\n",
    "def compute_cifar10_mean_std(root: str = CFG.data_root) -> Tuple[Tuple[float,...], Tuple[float,...]]:\n",
    "    tmp = torchvision.datasets.CIFAR10(root, train=True, download=True, transform=T.ToTensor())\n",
    "    loader = DataLoader(tmp, batch_size=512, shuffle=False, num_workers=CFG.num_workers)\n",
    "    n = 0vc vfbcxzZ\n",
    "    mean = torch.zeros(3)\n",
    "    M2 = torch.zeros(3)  # for online variance (Welford)\n",
    "    for x, _ in loader:\n",
    "        b = x.shape[0]\n",
    "        x = x.view(b, 3, -1)\n",
    "        batch_mean = x.mean(dim=(0, 2))\n",
    "        batch_var = x.var(dim=(0, 2), unbiased=False)\n",
    "        if n == 0:\n",
    "            mean = batch_mean\n",
    "            M2 = batch_var\n",
    "            n = b\n",
    "        else:\n",
    "            new_n = n + b\n",
    "            delta = batch_mean - mean\n",
    "            mean = mean + delta * (b / new_n)\n",
    "            M2 = (n * (M2 + (delta**2) * (b / new_n))) / new_n + (b * batch_var) / new_n\n",
    "            n = new_n\n",
    "    std = torch.sqrt(M2)\n",
    "    return tuple(mean.tolist()), tuple(std.tolist())\n",
    "\n",
    "\n",
    "# Build datasets and dataloaders\n",
    "\n",
    "def make_dataloaders(batch_size: int = CFG.batch_size) -> Tuple[DataLoader, DataLoader, Tuple[float,...], Tuple[float,...]]:\n",
    "    mean, std = compute_cifar10_mean_std()\n",
    "    # Q1.1: use transforms.Normalize with computed mean/std\n",
    "    train_tf = T.Compose([\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    train_set = torchvision.datasets.CIFAR10(CFG.data_root, train=True, download=False, transform=train_tf)\n",
    "    test_set  = torchvision.datasets.CIFAR10(CFG.data_root, train=False, download=True, transform=test_tf)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=CFG.num_workers, pin_memory=CFG.pin_memory)\n",
    "    test_loader  = DataLoader(test_set,  batch_size=256, shuffle=False, num_workers=CFG.num_workers, pin_memory=CFG.pin_memory)\n",
    "    return train_loader, test_loader, mean, std\n",
    "\n",
    "# Q1.2 + Q1.3 (basic inspection helper)\n",
    "\n",
    "def describe_dataset(train_loader: DataLoader, test_loader: DataLoader):\n",
    "    n_train = len(train_loader.dataset)\n",
    "    n_test = len(test_loader.dataset)\n",
    "    print(f\"Train size: {n_train}, Test size: {n_test}\")\n",
    "    print(\"Classes:\", CIFAR10_CLASSES)\n",
    "\n",
    "# =====================\n",
    "# 2) Visualization (Q3)\n",
    "# =====================\n",
    "\n",
    "def show_grid(loader: DataLoader, nrow: int = 8, ncol: int = 8, unnormalize: Optional[Tuple[Tuple[float,...], Tuple[float,...]]] = None, savepath: str = \"outputs/visual_grid.png\"):\n",
    "    x, y = next(iter(loader))\n",
    "    x = x[: nrow*ncol]\n",
    "    y = y[: nrow*ncol]\n",
    "    if unnormalize is not None:\n",
    "        mean, std = [torch.tensor(v).view(1, -1, 1, 1) for v in unnormalize]\n",
    "        x = x * std + mean\n",
    "    grid = torchvision.utils.make_grid(x, nrow=ncol)\n",
    "    npimg = grid.permute(1, 2, 0).detach().cpu().numpy()\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.imshow(npimg)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"CIFAR-10 sample grid\")\n",
    "    plt.savefig(savepath, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved visualization to {savepath}\")\n",
    "\n",
    "# ======================================\n",
    "# 3) Models (Q4–Q7): CNN (+BN toggle) and MLP\n",
    "# ======================================\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    # For Q7 (initialize and move to device)\n",
    "    def __init__(self, hidden: int = 512, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*32*32, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, use_bn: bool = True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True)]\n",
    "        if use_bn:\n",
    "            layers.insert(1, nn.BatchNorm2d(out_ch))\n",
    "        layers += [nn.Conv2d(out_ch, out_ch, 3, padding=1), nn.ReLU(inplace=True)]\n",
    "        if use_bn:\n",
    "            layers.insert(-1, nn.BatchNorm2d(out_ch))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"Q4/Q5/Q6: customizable CNN with optional BatchNorm.\n",
    "    Args:\n",
    "      channels: list like [64, 128, 256]\n",
    "      use_bn: add BatchNorm after conv layers (Q5)\n",
    "    \"\"\"\n",
    "    def __init__(self, channels: List[int] = [64,128,256], use_bn: bool = True, num_classes: int = 10):\n",
    "        super().__init__()\n",
    "        c = [3] + channels\n",
    "        blocks = []\n",
    "        for i in range(len(channels)):\n",
    "            blocks += [ConvBlock(c[i], c[i+1], use_bn), nn.MaxPool2d(2)]\n",
    "        self.features = nn.Sequential(*blocks)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(channels[-1], num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# Hardware info (Q3.4) + Q7: initialize and move models to device\n",
    "\n",
    "def print_hardware_info():\n",
    "    print(\"Device:\", DEVICE)\n",
    "    if DEVICE.type == 'cuda':\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    elif DEVICE.type == 'mps':\n",
    "        print(\"Apple Silicon GPU via MPS backend\")\n",
    "    else:\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "# ================================\n",
    "# 4) Training Utilities (Q8–Q9)\n",
    "# ================================\n",
    "\n",
    "def make_optimizer(params, name: str = \"sgd\", lr: float = 0.1, momentum: float = 0.9, weight_decay: float = 5e-4):\n",
    "    name = name.lower()\n",
    "    if name == \"sgd\":\n",
    "        return SGD(params, lr=lr, weight_decay=weight_decay)\n",
    "    elif name in (\"sgdm\", \"sgd_momentum\", \"sgd+momentum\"):\n",
    "        return SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {name}\")\n",
    "\n",
    "\n",
    "def make_scheduler(optimizer, name: Optional[str] = None, epochs: int = 30):\n",
    "    if name is None:\n",
    "        return None\n",
    "    name = name.lower()\n",
    "    if name in (\"cosine\", \"cosineannealinglr\"):\n",
    "        return CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    elif name in (\"step\", \"steplr\"):\n",
    "        return StepLR(optimizer, step_size=max(1, epochs//3), gamma=0.1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler: {name}\")\n",
    "\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module):\n",
    "    model.eval()\n",
    "    tot_loss, tot_acc, n = 0.0, 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        b = y.size(0)\n",
    "        tot_loss += loss.item() * b\n",
    "        tot_acc  += (logits.argmax(1) == y).float().sum().item()\n",
    "        n += b\n",
    "    return tot_loss/n, tot_acc/n\n",
    "\n",
    "\n",
    "def train(model: nn.Module, train_loader: DataLoader, test_loader: DataLoader,\n",
    "          epochs: int = 10, lr: float = 0.1, optimizer_name: str = \"sgdm\",\n",
    "          scheduler_name: Optional[str] = \"cosine\"):\n",
    "    \"\"\"Q4.3/4.4 + logging for Q5.1. Returns history dict.\"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optim = make_optimizer(model.parameters(), optimizer_name, lr=lr)\n",
    "    sched = make_scheduler(optim, scheduler_name, epochs)\n",
    "\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        ep_loss, ep_acc, n = 0.0, 0.0, 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE, non_blocking=True), y.to(DEVICE, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            with torch.no_grad():\n",
    "                b = y.size(0)\n",
    "                ep_loss += loss.item() * b\n",
    "                ep_acc  += (logits.argmax(1) == y).float().sum().item()\n",
    "                n += b\n",
    "        if sched is not None:\n",
    "            sched.step()\n",
    "        tr_loss, tr_acc = ep_loss/n, ep_acc/n\n",
    "        te_loss, te_acc = evaluate(model, test_loader, criterion)\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"test_loss\"].append(te_loss)\n",
    "        history[\"test_acc\"].append(te_acc)\n",
    "        print(f\"Epoch {ep:02d}/{epochs} | train loss {tr_loss:.3f} acc {tr_acc*100:.1f}% | test loss {te_loss:.3f} acc {te_acc*100:.1f}%\")\n",
    "    return history\n",
    "\n",
    "# ==========================\n",
    "# 5) Plotting & Comparison\n",
    "# ==========================\n",
    "\n",
    "def plot_curves(histories: Dict[str, Dict[str, List[float]]], title: str, save_prefix: str):\n",
    "    # Q5.1: plot loss and accuracy per epoch\n",
    "    # One plot per metric to keep them readable\n",
    "    e = len(next(iter(histories.values()))[\"train_loss\"]) if histories else 0\n",
    "    # Loss\n",
    "    plt.figure()\n",
    "    for name, h in histories.items():\n",
    "        plt.plot(range(1, e+1), h[\"train_loss\"], label=f\"{name} - train\")\n",
    "        plt.plot(range(1, e+1), h[\"test_loss\"], label=f\"{name} - test\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(title + \" — Loss\"); plt.legend();\n",
    "    lp = f\"outputs/{save_prefix}_loss.png\"; plt.savefig(lp, bbox_inches='tight'); plt.close(); print(\"Saved:\", lp)\n",
    "    # Accuracy\n",
    "    plt.figure()\n",
    "    for name, h in histories.items():\n",
    "        plt.plot(range(1, e+1), [a*100 for a in h[\"train_acc\"]], label=f\"{name} - train\")\n",
    "        plt.plot(range(1, e+1), [a*100 for a in h[\"test_acc\"]], label=f\"{name} - test\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(title + \" — Accuracy\"); plt.legend();\n",
    "    ap = f\"outputs/{save_prefix}_acc.png\"; plt.savefig(ap, bbox_inches='tight'); plt.close(); print(\"Saved:\", ap)\n",
    "\n",
    "# =====================================\n",
    "# 6) Experiments for Questions 10–12\n",
    "# =====================================\n",
    "\n",
    "def run_all(epochs: int = 10, batch_size: int = 128, lr: float = 0.1):\n",
    "    # Make data\n",
    "    train_loader, test_loader, mean, std = make_dataloaders(batch_size)\n",
    "    describe_dataset(train_loader, test_loader)  # (Q2)\n",
    "    show_grid(train_loader, unnormalize=(mean, std))  # (Q3)\n",
    "    print_hardware_info()  # (Q3.4)\n",
    "\n",
    "    # Q7: Initialize MLP and move to device\n",
    "    mlp = SimpleMLP().to(DEVICE)\n",
    "\n",
    "    # Q4/5/6: CNNs with and without BatchNorm (and customizable channels)\n",
    "    cnn_bn    = SimpleCNN(channels=[64,128,256], use_bn=True).to(DEVICE)\n",
    "    cnn_nobn  = SimpleCNN(channels=[64,128,256], use_bn=False).to(DEVICE)\n",
    "\n",
    "    # ---- Train a lightweight set of epochs so the assignment can run quickly ----\n",
    "    # You can increase epochs for better accuracy if you have GPU time.\n",
    "    histories = {}\n",
    "\n",
    "    # (Q10) MLP vs CNN\n",
    "    print(\"\\n[Q10] Training MLP vs CNN (BN)\")\n",
    "    histories['MLP']    = train(mlp,    train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    histories['CNN+BN'] = train(cnn_bn, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    plot_curves({k: histories[k] for k in ['MLP','CNN+BN']}, title='MLP vs CNN', save_prefix='q10_mlp_vs_cnn')\n",
    "\n",
    "    # (Q11) CNN with vs without BatchNorm\n",
    "    print(\"\\n[Q11] Training CNN with vs without BatchNorm\")\n",
    "    # re-init models so they start fresh\n",
    "    cnn_bn    = SimpleCNN(channels=[64,128,256], use_bn=True).to(DEVICE)\n",
    "    cnn_nobn  = SimpleCNN(channels=[64,128,256], use_bn=False).to(DEVICE)\n",
    "    histories['CNN+BN_2']   = train(cnn_bn,   train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    histories['CNN_noBN_2'] = train(cnn_nobn, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    plot_curves({'CNN+BN': histories['CNN+BN_2'], 'CNN no BN': histories['CNN_noBN_2']}, title='CNN: BN vs no BN', save_prefix='q11_bn_vs_nobn')\n",
    "\n",
    "    # (Q12) Optimizers comparison on the same CNN architecture\n",
    "    print(\"\\n[Q12] Optimizer comparison (SGD vs SGD+Momentum)\")\n",
    "    cnn_opt_a = SimpleCNN(channels=[64,128,256], use_bn=True).to(DEVICE)\n",
    "    cnn_opt_b = SimpleCNN(channels=[64,128,256], use_bn=True).to(DEVICE)\n",
    "    histories['CNN_SGD']  = train(cnn_opt_a, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgd',  scheduler_name='cosine')\n",
    "    histories['CNN_SGDM'] = train(cnn_opt_b, train_loader, test_loader, epochs=epochs, lr=lr, optimizer_name='sgdm', scheduler_name='cosine')\n",
    "    plot_curves({'CNN SGD': histories['CNN_SGD'], 'CNN SGD+Momentum': histories['CNN_SGDM']}, title='Optimizers on CNN', save_prefix='q12_optims')\n",
    "\n",
    "    print(\"\\nDone. Figures saved under ./outputs. Use these when answering Q13 (overfitting signs, factors, etc.).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example fast run for CPU users; raise epochs to 20–30 on GPU for nicer curves\n",
    "    run_all(epochs=5, batch_size=CFG.batch_size, lr=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
